{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score, add_hist\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib.patches import Patch\n",
    "import webcolors\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from data_api import CustomDataset\n",
    "\n",
    "batch_size = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# TODO\n",
    "# 모델 지정\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"efficientnet-b5\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=11,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "# TODO\n",
    "# 저장할 json 파일 이름 지정\n",
    "pseudo_labeling_name = 'model_10_threshold_75'\n",
    "\n",
    "# TODO\n",
    "# 체크포인트 경로 지정\n",
    "model_path = '/opt/ml/segmentation/baseline_code/saved/DeepLabV3Plus_efficientnet-b5_best_model.pt'\n",
    "dataset_path  = '/opt/ml/segmentation/input/data'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "state_dict = checkpoint.state_dict()\n",
    "# state_dict = checkpoint['net']\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    ToTensorV2()\n",
    "    ])\n",
    "\n",
    "test_dataset = CustomDataset(annotation='test.json', mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616\n"
     ]
    }
   ],
   "source": [
    "# json 파일 불러오기\n",
    "\n",
    "submission = pd.read_csv('/opt/ml/segmentation/baseline_code/submission/sample_submission.csv', index_col=None)\n",
    "teain_json_dir = os.path.join(\"/opt/ml/segmentation/input/data/train.json\")\n",
    "test_json_dir = os.path.join(\"/opt/ml/segmentation/input/data/test.json\")\n",
    "\n",
    "with open(teain_json_dir, \"r\", encoding=\"utf8\") as outfile:\n",
    "    json_train_data = json.load(outfile)\n",
    "    train_data = copy.deepcopy(json_train_data)\n",
    "with open(test_json_dir, \"r\", encoding=\"utf8\") as outfile:\n",
    "    test_data = json.load(outfile)\n",
    "\n",
    "last_id = train_data['images'][-1]['id']\n",
    "print(last_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/819 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합치기전 2617 장\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 149/819 [00:08<00:38, 17.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len2 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 171/819 [00:10<00:38, 16.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len2 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 275/819 [00:15<00:27, 19.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len2 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 318/819 [00:17<00:31, 15.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len2 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 381/819 [00:21<00:20, 21.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len4 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 401/819 [00:22<00:24, 17.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len2 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 435/819 [00:24<00:20, 18.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len4 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 509/819 [00:28<00:18, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len4 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 616/819 [00:34<00:13, 15.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len4 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 661/819 [00:36<00:08, 18.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len4 raises arror\n",
      "len2 raises arror\n",
      "len2 raises arror\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 819/819 [00:45<00:00, 18.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친후 2912 장\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "hahahoho = np.array([])\n",
    "file_name_list = []\n",
    "anno_id_num = 1\n",
    "img_id_num = 1\n",
    "threshold = 7.5\n",
    "\n",
    "print('합치기전', len(train_data['images']), '장')\n",
    "with torch.no_grad():\n",
    "    for step, (imgs, image_infos) in enumerate(tqdm(test_loader)):\n",
    "        \n",
    "        outs = model(torch.stack(imgs).to(device))\n",
    "        mean = torch.mean(torch.max(outs, dim=1)[0])\n",
    "        mean = mean.cpu().numpy()\n",
    "        hahahoho = np.append(hahahoho, mean)\n",
    "        if mean < threshold: continue # 픽셀 당 점수 평균이 threshold를 넘기지 못하면 제외\n",
    "        oms = torch.argmax(outs, dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "        # break\n",
    "        image_id = test_data[\"images\"][step]\n",
    "        file_name = image_id[\"file_name\"]\n",
    "        for seg_class in range(1, 11): # 1:genetral_trash 부터 10 clothing까지\n",
    "\n",
    "            temp_mask = copy.deepcopy(oms)\n",
    "            temp_mask[temp_mask != seg_class] = 0 # 현재 클래스와 다른 것들은 다 0 처리\n",
    "            temp_mask[temp_mask == seg_class] = 1 # binary 이미지로 만들어주기 위함\n",
    "            if len(np.unique(temp_mask)) == 1: continue # 해당 클래스 없으면 pass\n",
    "\n",
    "            anno = {}\n",
    "            anno['id'] = 21115 + anno_id_num\n",
    "            anno['category_id'] = seg_class\n",
    "            anno['image_id'] = img_id_num + last_id\n",
    "            seg_annos = []\n",
    "\n",
    "            temp_mask = temp_mask.astype(np.uint8)\n",
    "            temp_mask_boundaries, hierarchy  = cv2.findContours(temp_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE) # 경계만 추출\n",
    "            \n",
    "            for temp_mask_boundary in temp_mask_boundaries:\n",
    "                seg_anno = np.reshape(temp_mask_boundary, (-1)).tolist()\n",
    "                seg_annos += seg_anno\n",
    "            if len(seg_annos) in [2, 4]: # segmentation의 길이가 2나 4이면 mmseg 폴더 나눌 때 에러가 일어남\n",
    "                print(f'len{len(seg_annos)} raises arror')\n",
    "                continue\n",
    "            anno['segmentation'] = [seg_annos]\n",
    "\n",
    "            train_data['annotations'].append(anno)\n",
    "\n",
    "            anno_id_num += 1\n",
    "            \n",
    "        image_id['id'] = img_id_num + last_id\n",
    "        # print(image_id)\n",
    "        # print(anno['image_id'], anno['id'])\n",
    "        train_data['images'].append(image_id)\n",
    "        img_id_num += 1\n",
    "\n",
    "print('합친후', len(train_data['images']), '장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   2   4   6  13  14  34  70 164 217 193  88]\n",
      "[0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5 5.  5.5 6.  6.5 7.  7.5 8.  8.5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "hist, bin_edges = np.histogram(hahahoho, np.arange(start=0, stop=9, step=0.5))\n",
    "hist = np.insert(hist, 0, 0, axis=0)\n",
    "print(hist)\n",
    "print(bin_edges)\n",
    "plt.bar(bin_edges, height=hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/opt/ml/segmentation/input/data/{pseudo_labeling_name}.json', 'w') as fp:\n",
    "    json.dump(train_data, fp)\n",
    "\n",
    "# pseudo label된 json 파일을 SMP에서 사용할 경우 여기까지만 하고 json 파일을 바로 쓰시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (pseudo labeling된 데이터를 MMSegmentation에 사용할 경우) 이미지 폴더 재생성, mask 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/segmentation/input/data/model_10_threshold_75.json\n",
      "/opt/ml/segmentation/input/data/val.json\n"
     ]
    }
   ],
   "source": [
    "# 이미지 폴더 재생성, 재분배 (images 폴더 생성)\n",
    "import shutil\n",
    "\n",
    "DATAROOT = \"/opt/ml/segmentation/input/data\"\n",
    "# trainjson 이름을 pseudo labeling 한 json 파일로 바꿔줍니다.\n",
    "TRAINJSON = os.path.join(DATAROOT,f\"{pseudo_labeling_name}.json\")\n",
    "VALIDJSON = os.path.join(DATAROOT,\"val.json\")\n",
    "TESTJSON = os.path.join(DATAROOT,\"test.json\")\n",
    "\n",
    "'''\n",
    "Redistribution image by train/valid/test \n",
    "\n",
    "rename img by img_id\n",
    "'''\n",
    "def _rename_images(json_dir, image_dir):\n",
    "\twith open(json_dir, \"r\", encoding=\"utf8\") as outfile:\n",
    "\t\tjson_data = json.load(outfile)\n",
    "\timage_datas = json_data[\"images\"]\n",
    "\n",
    "\tfor image_data in image_datas:\n",
    "\n",
    "\t\tshutil.copyfile(os.path.join(DATAROOT, image_data['file_name']), os.path.join(image_dir,f\"{image_data['id']:04}.jpg\"))\n",
    "\n",
    "'''\n",
    "Wrap func\n",
    "'''\n",
    "def make(json,path):\n",
    "\timagePath = f'/opt/ml/segmentation/input/{pseudo_labeling_name}/images/'+path\n",
    "\n",
    "\tos.makedirs(imagePath, exist_ok=True)\n",
    "\t_rename_images(json,imagePath)\n",
    "\n",
    "\n",
    "'''\n",
    "Main\n",
    "'''\n",
    "def __main__():\n",
    "\n",
    "\tmake(TRAINJSON, 'training')\n",
    "\tmake(VALIDJSON, 'validation')\n",
    "\n",
    "__main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.41s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.93s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = A.Compose([\n",
    "                            A.Resize(512,512),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "train_dataset = CustomDataset(annotation=f\"{pseudo_labeling_name}.json\", mode='train', transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_52479/3334506756.py\", line 73, in collate_fn\n    return tuple(zip(*batch))\nTypeError: zip argument #1 must support iteration\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52479/3279716359.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/opt/ml/segmentation/input/{pseudo_labeling_name}/annotations/validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_infos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mimage_infos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# 파일 경로를 변경해주세요! .png 형식으로 저장해야 합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_52479/3334506756.py\", line 73, in collate_fn\n    return tuple(zip(*batch))\nTypeError: zip argument #1 must support iteration\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f'/opt/ml/segmentation/input/{pseudo_labeling_name}/annotations/training'):\n",
    "    os.makedirs(f'/opt/ml/segmentation/input/{pseudo_labeling_name}/annotations/training')\n",
    "\n",
    "for idx, (imgs, masks, image_infos) in enumerate(train_loader):\n",
    "    image_infos = image_infos[0]\n",
    "    # 파일 경로를 변경해주세요! .png 형식으로 저장해야 합니다.  \n",
    "    # 새로 나뉜 이미지를 image_id로 저장해두어서 마찬가지로 annotation도 image_id로 저장합니다. \n",
    "    # TODO redist_pseudo 파일에서 설정한 폴더 이름으로 경로 변경\n",
    "    file_dir = f\"/opt/ml/segmentation/input/{pseudo_labeling_name}/annotations/training/{image_infos['id']:04}.png\"\n",
    "\n",
    "    masks = masks[0].numpy()\n",
    "    \n",
    "    cv2.imwrite(file_dir, masks)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d36e052b391be8c28b05838ade06426769a29575d5fe21a7bc69c7dec0c04c06"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('segmentation': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
